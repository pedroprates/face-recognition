{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:09:38.601371Z",
     "start_time": "2018-10-19T02:09:37.478099Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import progressbar\n",
    "from sklearn.model_selection import KFold\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the pairs\n",
    "\n",
    "Getting the same pairs, stored on `pairs.txt`, as the validation on the [Facenet](https://github.com/davidsandberg/facenet). \n",
    "\n",
    "The `pairs.txt` file is divided between two use cases:\n",
    "* Lines with one *name* and two *numbers* means positive samples, i.e., we will compare two different images of the same user on the test set.\n",
    "* Lines with two *names* and two *numbers* means negative samples, i.e., we will compare two different photos, composed by the name of the first user and number of picture of the first user vs. the name of second user and number of the second user picture.\n",
    "\n",
    "This information is used to prepare the **helper variables**, that will be extremely important to run the validation.\n",
    "\n",
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:11:12.954707Z",
     "start_time": "2018-10-19T02:11:12.946563Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_pairs(path):\n",
    "    pairs=[]\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            pair = line.strip().split()\n",
    "            pairs.append(pair)\n",
    "            \n",
    "    return np.array(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:11:13.633983Z",
     "start_time": "2018-10-19T02:11:13.620307Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_path(lfw_dir, pair, output):\n",
    "    if len(pair) == 3:\n",
    "        # Use case -- same\n",
    "        path0 = os.path.join(lfw_dir, pair[0], output, pair[0] + '_' + '%04d' % int(pair[1])) + '.npy'\n",
    "        path1 = os.path.join(lfw_dir, pair[0], output, pair[0] + '_' + '%04d' % int(pair[2])) + '.npy'\n",
    "        issame = True\n",
    "    elif len(pair) == 4:\n",
    "        # Use case -- differente\n",
    "        path0 = os.path.join(lfw_dir, pair[0], output, pair[0] + '_' + '%04d' % int(pair[1])) + '.npy'\n",
    "        path1 = os.path.join(lfw_dir, pair[2], output, pair[2] + '_' + '%04d' % int(pair[3])) + '.npy'\n",
    "        issame = False\n",
    "    return path0, path1, issame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:11:15.702797Z",
     "start_time": "2018-10-19T02:11:15.685360Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paths(lfw_dir, pairs, output):\n",
    "    nrof_skipped_pairs = 0\n",
    "    path_list = []\n",
    "    issame_list = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        path0, path1, issame = create_path(lfw_dir, pair, output)\n",
    "            \n",
    "        if os.path.exists(path0) and os.path.exists(path1):\n",
    "            path_list += (path0, path1)\n",
    "            issame_list.append(issame)\n",
    "        else:\n",
    "            nrof_skipped_pairs += 1\n",
    "            \n",
    "    if nrof_skipped_pairs > 0:\n",
    "        print('There was %d skipped pairs.' % nrof_skipped_pairs)\n",
    "        \n",
    "    return path_list, issame_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "\n",
    "Running the code from the *helper functions* to obtain the **global variables** which will manage the entire **validation** session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:11:18.183626Z",
     "start_time": "2018-10-19T02:11:18.177852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the global variables, used over this code\n",
    "pairs_path = '/Users/pedroprates/Google Drive/FaceRecognition/data/pairs.txt'\n",
    "lfw_path = '/Users/pedroprates/Google Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:11:18.621064Z",
     "start_time": "2018-10-19T02:11:18.603023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs.txt was successfully read and it was retrieved a ndarray with shape (6000,)\n"
     ]
    }
   ],
   "source": [
    "pairs = read_pairs(pairs_path)\n",
    "print(\"pairs.txt was successfully read and it was retrieved a ndarray with shape\", str(pairs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:03:50.649344Z",
     "start_time": "2018-10-19T03:03:50.005098Z"
    }
   },
   "outputs": [],
   "source": [
    "path_list, actual_issame = get_paths(lfw_path, pairs, 'mobilenetv2_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the validation\n",
    "\n",
    "Now we are going to run the validation based on the `npy` paths that we've acquired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:09:55.478724Z",
     "start_time": "2018-10-19T02:09:55.456416Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance(embeddings1, embeddings2, distance_metric='euclidean'):\n",
    "    \"\"\" Calculate the distance between two embeddings. Currently working with euclidean and cosine similarity. \n",
    "\n",
    "        :param embeddings1: First embedding\n",
    "        :param embeddings2: Second embedding\n",
    "        :param distance_metric: Distance metric to be used to make the calculation. Should be either: 'euclidean' or 'cosine'\n",
    "\n",
    "        :returns: The distance between the `embeddings1` and `embeddings2`\n",
    "    \"\"\"\n",
    "    assert distance_metric in ['euclidean', 'cosine'], \"The distance metric should be either 'euclidean' or 'cosine'\"\n",
    "\n",
    "    if distance_metric == 'euclidean':\n",
    "        diff = np.subtract(embeddings1, embeddings2)\n",
    "        dist = np.sum(np.square(diff), 1)\n",
    "\n",
    "    elif distance_metric == 'cosine':\n",
    "        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n",
    "        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
    "        similarity = dot / norm\n",
    "        dist = np.arccos(similarity) / math.pi\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:09:56.113967Z",
     "start_time": "2018-10-19T02:09:56.090516Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_embeddings(paths):\n",
    "    nrof_skips = 0\n",
    "    bt_size = len(paths)\n",
    "    embeddings = np.zeros((bt_size, 512))\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        if not os.path.exists(path):\n",
    "            nrof_skips += 1\n",
    "            continue\n",
    "        emb = np.load(path)\n",
    "        embeddings[i, :] = emb\n",
    "        \n",
    "    if nrof_skips > 0:\n",
    "        print(\"There are %d files that weren't found\" % nrof_skips)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:09:57.077919Z",
     "start_time": "2018-10-19T02:09:57.054778Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "    \n",
    "    tpr = 0 if tp + fn == 0 else float(tp) / (tp + fn)\n",
    "    fpr = 0 if fp + tn == 0 else float(fp) / (fp + tn)\n",
    "    acc = float(tp+tn) / (tp+fp+tn+fn)\n",
    "    \n",
    "    return tpr, fpr, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:09:58.431040Z",
     "start_time": "2018-10-19T02:09:58.350854Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_roc(thresholds,\n",
    "                  embeddings1,\n",
    "                  embeddings2,\n",
    "                  actual_issame,\n",
    "                  distance_metric='cosine',\n",
    "                  subtract_mean=True,\n",
    "                  nrof_folds=10):\n",
    "\n",
    "    assert embeddings1.shape[0] == embeddings2.shape[0]\n",
    "    assert embeddings1.shape[1] == embeddings2.shape[1]\n",
    "\n",
    "    kfolds = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "\n",
    "    tprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "    accuracy = np.zeros(nrof_folds)\n",
    "\n",
    "    indices = np.arange(nrof_pairs)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(kfolds.split(indices)):\n",
    "        if subtract_mean:\n",
    "            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]], axis=0))\n",
    "        else:\n",
    "            mean = 0\n",
    "\n",
    "        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n",
    "        acc_train = np.zeros(nrof_thresholds)\n",
    "\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n",
    "        best_threshold_idx = np.argmax(acc_train)\n",
    "\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\n",
    "                                                                                                 dist[test_set],\n",
    "                                                                                                 actual_issame[test_set])\n",
    "        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_idx],\n",
    "                                                      dist[test_set],\n",
    "                                                      actual_issame[test_set])\n",
    "\n",
    "        tpr = np.mean(tprs, 0)\n",
    "        fpr = np.mean(fprs, 0)\n",
    "\n",
    "    return tpr, fpr, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:09:59.095440Z",
     "start_time": "2018-10-19T02:09:59.083289Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(embeddings, actual_issame, distance_metric='cosine', subtract_mean=False):\n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    embeddings1 = embeddings[0::2]\n",
    "    embeddings2 = embeddings[1::2]\n",
    "    tpr, fpr, acc = calculate_roc(thresholds, \n",
    "                                  embeddings1, \n",
    "                                  embeddings2, \n",
    "                                  np.array(actual_issame), \n",
    "                                  distance_metric=distance_metric, \n",
    "                                  subtract_mean=subtract_mean,\n",
    "                                  nrof_folds=10)\n",
    "    return tpr, fpr, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:04:23.185718Z",
     "start_time": "2018-10-19T03:04:16.473249Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = load_embeddings(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:04:26.227439Z",
     "start_time": "2018-10-19T03:04:24.367553Z"
    }
   },
   "outputs": [],
   "source": [
    "tpr, fpr, acc_mobile = evaluate(embeddings, actual_issame, subtract_mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:04:27.510797Z",
     "start_time": "2018-10-19T03:04:27.471705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60166667, 0.56166667, 0.56333333, 0.54      , 0.57      ,\n",
       "       0.57666667, 0.565     , 0.535     , 0.55166667, 0.53333333])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:10:17.034905Z",
     "start_time": "2018-10-19T02:10:17.012009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99      , 0.97833333, 0.97333333, 0.98      , 0.99      ,\n",
       "       0.98833333, 0.98      , 0.99166667, 0.98833333, 0.99166667])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:23:10.580028Z",
     "start_time": "2018-10-17T01:23:10.575177Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import src.facenet as facenet\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:14:30.001169Z",
     "start_time": "2018-10-17T01:14:29.994080Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_image(path):\n",
    "    original_image = misc.imread(path)\n",
    "    image = original_image.reshape((1, *original_image.shape))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:13:23.334234Z",
     "start_time": "2018-10-17T01:13:23.322248Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(image, sess):\n",
    "    images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "    phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "    # Get embeddings\n",
    "    embeddings_size = embeddings.get_shape()[1]\n",
    "    feed_dict = { images_placeholder: image, phase_train_placeholder: False }\n",
    "    embeddings_array = np.zeros((1, embeddings_size))\n",
    "    embeddings_array = sess.run(embeddings, feed_dict=feed_dict)\n",
    "\n",
    "    return embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:23:12.999330Z",
     "start_time": "2018-10-17T01:23:12.980737Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(paths):\n",
    "    nrof_skips = 0\n",
    "    bt_size = len(paths)\n",
    "    embeddings = np.zeros((bt_size, 512))\n",
    "    sess=tf.Session()\n",
    "    facenet.load_model('/Users/pedroprates/Google Drive/FaceRecognition/models/facenet/20180402-114759/20180402-114759.pb')\n",
    "    \n",
    "    for i, path in progressbar.progressbar(enumerate(paths)):\n",
    "        if not os.path.exists(path):\n",
    "            nrof_skips += 1\n",
    "            continue\n",
    "        \n",
    "        image = process_image(path)\n",
    "        emb = forward_propagation(image, sess)\n",
    "        embeddings[i, :] = emb\n",
    "        \n",
    "    if nrof_skips > 0:\n",
    "        print(\"There are %d files that weren't found\" % nrof_skips)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:17:43.568899Z",
     "start_time": "2018-10-17T01:17:43.509997Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paths(lfw_dir, pairs):\n",
    "    nrof_skipped_pairs = 0\n",
    "    path_list = []\n",
    "    issame_list = []\n",
    "    for pair in pairs:\n",
    "        if len(pair) == 3:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[1])))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[2])))\n",
    "            issame = True\n",
    "        elif len(pair) == 4:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[1])))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + '_' + '%04d' % int(pair[3])))\n",
    "            issame = False\n",
    "        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\n",
    "            path_list += (path0,path1)\n",
    "            issame_list.append(issame)\n",
    "        else:\n",
    "            nrof_skipped_pairs += 1\n",
    "    if nrof_skipped_pairs>0:\n",
    "        print('Skipped %d image pairs' % nrof_skipped_pairs)\n",
    "    \n",
    "    return path_list, issame_list\n",
    "\n",
    "\n",
    "def add_extension(path):\n",
    "    if os.path.exists(path+'.jpg'):\n",
    "        return path+'.jpg'\n",
    "    elif os.path.exists(path+'.png'):\n",
    "        return path+'.png'\n",
    "    else:\n",
    "        raise RuntimeError('No file \"%s\" with extension png or jpg.' % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:18:03.440233Z",
     "start_time": "2018-10-17T01:18:03.434700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the global variables, used over this code\n",
    "pairs_path = '/Users/pedroprates/Google Drive/FaceRecognition/data/pairs.txt'\n",
    "lfw_path = '/Users/pedroprates/Google Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:18:13.341898Z",
     "start_time": "2018-10-17T01:18:13.328310Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = read_pairs(pairs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:18:42.287089Z",
     "start_time": "2018-10-17T01:18:41.599045Z"
    }
   },
   "outputs": [],
   "source": [
    "path_list, actual_issame = get_paths(lfw_path, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T02:13:39.493519Z",
     "start_time": "2018-10-17T02:13:39.415386Z"
    }
   },
   "outputs": [],
   "source": [
    "embs = np.load('/Users/pedroprates/Google Drive/FaceRecognition/datasets/lfw_from_facenet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T02:13:49.442503Z",
     "start_time": "2018-10-17T02:13:49.434526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 512)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:39:25.697304Z",
     "start_time": "2018-10-17T01:23:14.798908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model filename: /Users/pedroprates/Google Drive/FaceRecognition/models/facenet/20180402-114759/20180402-114759.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ |#                                                  | 0 Elapsed Time: 0:00:00/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "| |                               #               | 11999 Elapsed Time: 0:16:05\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T02:14:05.362890Z",
     "start_time": "2018-10-17T02:14:04.305078Z"
    }
   },
   "outputs": [],
   "source": [
    "tpr, fpr, acc = evaluate(embs, actual_issame, subtract_mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T02:14:06.458641Z",
     "start_time": "2018-10-17T02:14:06.451784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98666667, 0.985     , 0.97833333, 0.98666667, 0.99      ,\n",
       "       0.99166667, 0.98666667, 0.99333333, 0.99333333, 0.99166667])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:41:01.804441Z",
     "start_time": "2018-10-17T01:41:01.781858Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "  \n",
    "    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n",
    "    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n",
    "    acc = float(tp+tn)/dist.size\n",
    "    return tpr, fpr, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T01:43:50.777139Z",
     "start_time": "2018-10-17T01:43:50.719193Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    assert(embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert(embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "    \n",
    "    tprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    accuracy = np.zeros((nrof_folds))\n",
    "    \n",
    "    indices = np.arange(nrof_pairs)\n",
    "    \n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        if subtract_mean:\n",
    "            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n",
    "        else:\n",
    "            mean = 0.0\n",
    "        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n",
    "        \n",
    "        # Find the best threshold for the fold\n",
    "        acc_train = np.zeros((nrof_thresholds))\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n",
    "        best_threshold_index = np.argmax(acc_train)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n",
    "        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n",
    "          \n",
    "        tpr = np.mean(tprs,0)\n",
    "        fpr = np.mean(fprs,0)\n",
    "    return tpr, fpr, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T09:22:42.783590Z",
     "start_time": "2018-10-17T09:22:42.723482Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric='cosine', subtract_mean=False):\n",
    "    # Calculate evaluation metrics\n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    embeddings1 = embeddings[0::2]\n",
    "    embeddings2 = embeddings[1::2]\n",
    "    tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2,\n",
    "        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    thresholds = np.arange(0, 4, 0.001)\n",
    "#     val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\n",
    "#         np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    return tpr, fpr, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T09:32:50.411479Z",
     "start_time": "2018-10-17T09:32:50.400678Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_number(imagename):\n",
    "    imagename = imagename.split('.')[0]\n",
    "    imagename = imagename.split('_')[-1]\n",
    "    \n",
    "    return int(imagename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T09:34:03.558843Z",
     "start_time": "2018-10-17T09:34:02.811348Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (5749 of 5749) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "lfw_dir = '/Users/pedroprates/Google Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160/'\n",
    "people = os.listdir(lfw_dir)\n",
    "people = list(filter(lambda x: os.path.isdir(os.path.join(lfw_dir, x)), people))\n",
    "\n",
    "images = []\n",
    "for person in progressbar.progressbar(people):\n",
    "    person_folder = os.path.join(lfw_dir, person)\n",
    "    list_pics = os.listdir(person_folder)\n",
    "    list_pics = list(filter(lambda x: x.endswith('.png') or x.endswith('.jpg'), list_pics))\n",
    "    \n",
    "    for pic in list_pics:\n",
    "        img_number = get_image_number(pic)\n",
    "        images += (person, img_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T09:43:59.534255Z",
     "start_time": "2018-10-17T09:43:59.503658Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = '/Users/pedroprates/Google Drive/FaceRecognition/data/all_dataset.txt'\n",
    "with open(output_dir, 'w') as f:\n",
    "    for i in range(0, len(images), 4):\n",
    "        to_write = images[i] + '\\t' + str(images[i+1]) + '\\t' \n",
    "        if (i+2) >= len(images):\n",
    "            to_write += images[i] + '\\t' + str(images[i+1])\n",
    "        else:\n",
    "            to_write += images[i+2] + '\\t' + str(images[i+3])\n",
    "            \n",
    "        to_write += '\\n'\n",
    "        f.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:03:59.877914Z",
     "start_time": "2018-10-17T10:03:59.872780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13233"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:04:36.437921Z",
     "start_time": "2018-10-17T10:04:36.321491Z"
    }
   },
   "outputs": [],
   "source": [
    "embs = np.load('/Users/pedroprates/Google Drive/FaceRecognition/datasets/all_lfw.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:05:14.084780Z",
     "start_time": "2018-10-17T10:05:14.078851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13234, 512)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:06:45.193341Z",
     "start_time": "2018-10-17T10:06:44.604985Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_path = '/Users/pedroprates/Google Drive/FaceRecognition/data/all_dataset.txt'\n",
    "pairs = read_pairs(pairs_path)\n",
    "path_list, actual_issame = get_paths(lfw_path, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:06:51.644974Z",
     "start_time": "2018-10-17T10:06:51.640645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13234"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:08:10.273816Z",
     "start_time": "2018-10-17T10:08:10.268993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pedroprates/Google Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160/German_Khan/German_Khan_0001.png'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:09:50.311651Z",
     "start_time": "2018-10-17T10:09:50.305552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pedroprates/Google Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160/German_Khan/output_resnet/German_Khan_0001.npy'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_path(path_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:11:24.991942Z",
     "start_time": "2018-10-17T10:11:24.981397Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_path(path, lfw_dir='/Users/pedroprates/Google Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160'):\n",
    "    path = path.split('/')\n",
    "    name = path[-2]\n",
    "    file = path[-1]\n",
    "    \n",
    "    file = file.split('.')[0] + '.npy'\n",
    "    \n",
    "    return os.path.join(lfw_dir, name, 'output_resnet', file), os.path.join(lfw_dir, name, 'output_resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:12:18.269400Z",
     "start_time": "2018-10-17T10:11:59.187898Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |  #                                            | 13233 Elapsed Time: 0:00:19\n"
     ]
    }
   ],
   "source": [
    "for index, embedding in progressbar.progressbar(enumerate(embs)):\n",
    "    embedding = embedding.reshape(1, *embedding.shape)\n",
    "    \n",
    "    save_path, folder_path = change_path(path_list[index])\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "        \n",
    "    np.save(save_path, embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
