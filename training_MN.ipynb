{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training_MN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["WMqlpf1y0Lu8","z8tigcbF0FVJ","BEbyII0lMPDb","_LHJHp7YMRpC","Obi_cHoS1FME","ro3OJtd7ADtc","4tyfZuLsgTkP","mzZh2TL26Bm8","7qb3oa3cGeA8","MlFJQFVdGhep"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WMqlpf1y0Lu8","colab_type":"text"},"cell_type":"markdown","source":["# Mount Google Drive"]},{"metadata":{"id":"kFGxFXGnh_f4","colab_type":"code","outputId":"ab0efdca-8680-472c-8c0c-8bdcd4ffc7f0","executionInfo":{"status":"ok","timestamp":1544706069919,"user_tz":120,"elapsed":5628,"user":{"displayName":"Pedro Prates","photoUrl":"https://lh6.googleusercontent.com/-ogAT-E4sfMA/AAAAAAAAAAI/AAAAAAAAAa4/9MA7SqDoyXw/s64/photo.jpg","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"z8tigcbF0FVJ","colab_type":"text"},"cell_type":"markdown","source":["# Loading Libraries"]},{"metadata":{"id":"og-F9IGK0QlP","colab_type":"text"},"cell_type":"markdown","source":["You should restart the runtime after running the cell below."]},{"metadata":{"id":"pYiE1fcoXV5O","colab_type":"code","outputId":"62102f92-ed02-459d-81ad-cd32d6522980","executionInfo":{"status":"ok","timestamp":1544706082427,"user_tz":120,"elapsed":7365,"user":{"displayName":"Pedro Prates","photoUrl":"https://lh6.googleusercontent.com/-ogAT-E4sfMA/AAAAAAAAAAI/AAAAAAAAAa4/9MA7SqDoyXw/s64/photo.jpg","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["!pip install progressbar2\n","!pip install --upgrade keras"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (3.38.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2) (1.11.0)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2) (2.3.0)\n","Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n","Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.5)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.6)\n"],"name":"stdout"}]},{"metadata":{"id":"L9ZTKKaE-hQD","colab_type":"code","outputId":"2361ceb6-6e05-4183-e840-16a2ef88062d","executionInfo":{"status":"ok","timestamp":1544706093768,"user_tz":120,"elapsed":1719,"user":{"displayName":"Pedro Prates","photoUrl":"https://lh6.googleusercontent.com/-ogAT-E4sfMA/AAAAAAAAAAI/AAAAAAAAAa4/9MA7SqDoyXw/s64/photo.jpg","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# Checking if the keras version is OK\n","import keras\n","\n","if keras.__version__  == '2.2.4':\n","  print('Version ok!')\n","else:\n","  print('Should be v2.2.4')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Version ok!\n"],"name":"stdout"}]},{"metadata":{"id":"eC6fM2cy_deJ","colab_type":"code","colab":{}},"cell_type":"code","source":["import progressbar\n","from scipy import misc\n","import numpy as np\n","import os\n","import math\n","from keras.applications.mobilenet_v2 import MobileNetV2\n","from keras.applications.mobilenet import MobileNet\n","from keras import Model\n","from keras.layers import Conv2D, BatchNormalization, ReLU, GlobalAveragePooling2D"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-eGcmW-O00ow","colab_type":"text"},"cell_type":"markdown","source":["# Loading the model\n","\n","The `imagenet` weights are not available right now, so the model its been loaded with the random weights from a normal distribution."]},{"metadata":{"id":"BEbyII0lMPDb","colab_type":"text"},"cell_type":"markdown","source":["## MobileNet V2"]},{"metadata":{"id":"EqGx0YGiV7dr","colab_type":"code","colab":{}},"cell_type":"code","source":["modelV2 = MobileNetV2(input_shape=(160, 160, 3),\n","           alpha=1,\n","           depth_multiplier=1,\n","           weights=None,\n","           include_top=False,\n","           pooling=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yGyN7pt2Rp2P","colab_type":"code","colab":{}},"cell_type":"code","source":["inp = modelV2.input\n","x = modelV2.layers[-4].output\n","\n","x = Conv2D(filters=512, kernel_size=(5,5), strides=1, padding=\"same\", activation=None, \n","           name=\"Conv_Last\", use_bias=False)(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","x = GlobalAveragePooling2D()(x)\n","\n","v2_model = Model(inputs=inp, outputs=x)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_LHJHp7YMRpC","colab_type":"text"},"cell_type":"markdown","source":["## MobileNet V1\n","\n","Loading the `MobileNetV1` implementation from Keras, with the weights pre-trained on the **ImageNet** classification challenge."]},{"metadata":{"id":"O5Vh60DeE4hK","colab_type":"code","outputId":"a557bc8f-4615-4b47-f34e-1c0ec088bcf0","executionInfo":{"status":"ok","timestamp":1544706107281,"user_tz":120,"elapsed":6882,"user":{"displayName":"Pedro Prates","photoUrl":"https://lh6.googleusercontent.com/-ogAT-E4sfMA/AAAAAAAAAAI/AAAAAAAAAa4/9MA7SqDoyXw/s64/photo.jpg","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["modelV1 = MobileNet(input_shape=(160, 160, 3),\n","                   alpha=1,\n","                   depth_multiplier=1,\n","                   weights='imagenet',\n","                   dropout=0,\n","                   include_top=False,\n","                   pooling=None)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_160_tf_no_top.h5\n","17227776/17225924 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"-6NSNdzEFw2p","colab_type":"code","colab":{}},"cell_type":"code","source":["inp = modelV1.input\n","x = modelV1.layers[-1].output\n","x = Conv2D(filters=512, kernel_size=(5, 5), strides=1, padding=\"same\", activation=None,\n","          name=\"Conv_Last\", use_bias=False)(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","x = GlobalAveragePooling2D()(x)\n","\n","v1_model = Model(inputs=inp, outputs=x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Obi_cHoS1FME","colab_type":"text"},"cell_type":"markdown","source":["## Summary\n","\n","Running the cell below it's an easy way to visualize each layer of the Convolution Neural Network, together with its input and output tensor shapes."]},{"metadata":{"id":"8Kjkl15kFs1a","colab_type":"code","colab":{}},"cell_type":"code","source":["modelV1.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OCyR4mptGVmQ","colab_type":"code","colab":{}},"cell_type":"code","source":["v1_model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ptnJTfepAHMM","colab_type":"code","colab":{}},"cell_type":"code","source":["modelV2.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q9e9megrP4rL","colab_type":"code","colab":{}},"cell_type":"code","source":["final_model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ro3OJtd7ADtc","colab_type":"text"},"cell_type":"markdown","source":["# Plotting model\n","\n","Responsible for plotting the loaded model for a better visualization of its parameters."]},{"metadata":{"id":"zi8-eMhOg7gK","colab_type":"code","colab":{}},"cell_type":"code","source":["# import graphviz\n","# import pydot\n","\n","!pip install pydot\n","!pip install graphviz"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h33hNa_8eSI7","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.utils import plot_model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LqST3ksCgtDy","colab_type":"code","colab":{}},"cell_type":"code","source":["plot_model(modelV2, to_file='/gdrive/My Drive/FaceRecognition/art/mobilenet_v2.png', show_shapes=True, show_layer_names=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4tyfZuLsgTkP","colab_type":"text"},"cell_type":"markdown","source":["# Input Data\n","\n","Getting a list of all the input images path. Before running this, check if the `lfw` folder already has the `input.npy` and `output.npy` files. If so, go to the **Splitting the Data** section."]},{"metadata":{"id":"a9HjgJCKgV6c","colab_type":"code","outputId":"903c1ced-c69d-4b40-f6e8-1889e8494e15","executionInfo":{"status":"error","timestamp":1539817877582,"user_tz":180,"elapsed":47540,"user":{"displayName":"Pedro Prates","photoUrl":"","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"cell_type":"code","source":["initial_path = '/gdrive/My Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160'\n","os.listdir(initial_path)\n","\n","dirs = [os.path.join(initial_path, d) for d in os.listdir(initial_path) if os.path.isdir(os.path.join(initial_path, d))]"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-098c64cab813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minitial_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/gdrive/My Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"Dl5BBpIvhmSu","colab_type":"code","colab":{}},"cell_type":"code","source":["import progressbar\n","\n","inputs = []\n","for d in progressbar.progressbar(dirs):\n","  for f in os.listdir(d):\n","    if f.endswith('png') or f.endswith('jpg') or f.endswith('jpeg'):\n","      inputs.append(os.path.join(d, f))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ORYK2hR3oWpO","colab_type":"code","colab":{}},"cell_type":"code","source":["outputs = []\n","\n","for inp in progressbar.progressbar(inputs):\n","  filename = inp.split('/')[-1]\n","  path = inp.split('/')[:-1]\n","  \n","  filename = 'output/' + filename.split('.')[0] + '.npy'\n","  path = '/'.join(path)\n","  \n","  outputs.append(os.path.join(path, filename))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HwXhS52to9ZT","colab_type":"code","colab":{}},"cell_type":"code","source":["inputs_np = np.array(inputs)\n","outputs_np = np.array(outputs)\n","\n","np.save('/gdrive/My Drive/FaceRecognition/datasets/lfw/input.npy', inputs_np)\n","np.save('/gdrive/My Drive/FaceRecognition/datasets/lfw/output.npy', outputs_np)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j49q-ivYiOmn","colab_type":"text"},"cell_type":"markdown","source":["# Splitting the Data\n","\n","Splitting the data on training, validation and test sets. It should follow the partition as 80% - 10% - 10%."]},{"metadata":{"id":"qDkkPZtpiP5B","colab_type":"code","colab":{}},"cell_type":"code","source":["input_data = np.load('/gdrive/My Drive/FaceRecognition/datasets/lfw/input_resnet.npy')\n","output_data = np.load('/gdrive/My Drive/FaceRecognition/datasets/lfw/output_resnet.npy')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OE66u3Acics3","colab_type":"code","colab":{}},"cell_type":"code","source":["nrof_samples = input_data.shape[0]\n","randomize = np.random.peyrmutation(nrof_samples)\n","inp = input_data[randomize]\n","out = output_data[randomize]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2_mvBCVOi9kC","colab_type":"code","colab":{}},"cell_type":"code","source":["training_size = np.floor(.8 * nrof_samples).astype(int)\n","val_size = (nrof_samples - training_size) // 2\n","\n","X_train = inp[:training_size]\n","y_train = out[:training_size]\n","X_val = inp[training_size:training_size+val_size]\n","y_val = out[training_size:training_size+val_size]\n","X_test = inp[training_size+val_size:]\n","y_test = out[training_size+val_size:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ecj4lceppmxO","colab_type":"text"},"cell_type":"markdown","source":["# Training Callback\n","\n","A way to visualize the training `loss` in real time. "]},{"metadata":{"id":"s0SPyF9Ppuvt","colab_type":"code","colab":{}},"cell_type":"code","source":["from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","\n","class TrainingPlot(keras.callbacks.Callback):\n","  # This function is called when the training begins\n","  def on_train_begin(self, logs={}):\n","    self.losses = []\n","    self.val_losses = []\n","    self.logs = []\n","    \n","  # This function is called at the end of each epoch\n","  def on_epoch_end(self, epoch, logs={}):\n","    # Append thel logs, losses to the lists\n","    self.logs.append(logs)\n","    self.losses.append(logs.get('loss'))\n","    self.val_losses.append(logs.get('val_loss'))\n","    \n","    # Before plotting ensure at least 2 epochs have passed\n","    if len(self.losses) > 1:\n","      # Clear the previous plot\n","      clear_output(wait=True)\n","      N = np.arange(0, len(self.losses))\n","      \n","      plt.style.use('seaborn')\n","      \n","      plt.figure()\n","      plt.plot(N, self.losses, label=\"train_loss\")\n","      plt.plot(N, self.val_losses, label=\"val_loss\")\n","      plt.title(\"Training & Val Loss [Epoch {}]\".format(epoch))\n","      plt.xlabel(\"Epoch #\")\n","      plt.ylabel(\"Loss\")\n","      plt.legend()\n","      plt.show()\n","      \n","plot_losses = TrainingPlot()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mCmme2_BrayQ","colab_type":"text"},"cell_type":"markdown","source":["# Training\n","\n","Distilling the network of the pre-trained [Facenet](https://github.com/davidsandberg/facenet) into a MobileNet v2."]},{"metadata":{"id":"mzZh2TL26Bm8","colab_type":"text"},"cell_type":"markdown","source":["## Loading the input generator\n","\n","`TCC_Generator` class has been developed to manage the input images through the CNN during training. It's responsible to split the training set into a series of batches (which its lengths has been previously defined), and then feed those batches (X and y, respectively) through the Mobile Net training architecture."]},{"metadata":{"id":"-ZDgJKPgrcyc","colab_type":"code","colab":{}},"cell_type":"code","source":["os.chdir('/gdrive/My Drive/FaceRecognition/src')\n","from data_generator import TCCGenerator"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5Ltry9CrrmRl","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 128\n","training_batch_generator = TCCGenerator(X_train, y_train, batch_size)\n","val_batch_generator = TCCGenerator(X_val, y_val, batch_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PuMeiRYv6-wG","colab_type":"text"},"cell_type":"markdown","source":["## Compile and Training the model\n","\n","Compile the model with the Adam optimizer and learning rate $0.001$. The loss function is **Mean Squared Error**, and the training will run for 40 epochs using 10 processes on different threads. "]},{"metadata":{"id":"7qb3oa3cGeA8","colab_type":"text"},"cell_type":"markdown","source":["### MobineNet V2"]},{"metadata":{"id":"8s0bWhKRrKnW","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import losses\n","\n","num_epochs = 100\n","queue_size = 12\n","\n","v2_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n","                   loss=losses.mean_squared_error)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BrxUchT8w1KA","colab_type":"code","outputId":"5dfff290-4fd1-4318-a492-51374d5e357f","executionInfo":{"status":"ok","timestamp":1539900527510,"user_tz":180,"elapsed":9925605,"user":{"displayName":"Pedro Prates","photoUrl":"","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":3417}},"cell_type":"code","source":["history = v2_model.fit_generator(generator=training_batch_generator,\n","                         epochs=num_epochs,\n","                         verbose=1,\n","                         validation_data=val_batch_generator,\n","                         max_queue_size=queue_size,\n","                         workers=10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","83/83 [==============================] - 1890s 23s/step - loss: 0.0230 - val_loss: 0.0045\n","Epoch 2/100\n","83/83 [==============================] - 85s 1s/step - loss: 0.0035 - val_loss: 0.0041\n","Epoch 3/100\n","83/83 [==============================] - 88s 1s/step - loss: 0.0020 - val_loss: 0.0025\n","Epoch 4/100\n","83/83 [==============================] - 84s 1s/step - loss: 0.0019 - val_loss: 0.0020\n","Epoch 5/100\n","83/83 [==============================] - 83s 1s/step - loss: 0.0019 - val_loss: 0.0019\n","Epoch 6/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0019 - val_loss: 0.0019\n","Epoch 7/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0019 - val_loss: 0.0019\n","Epoch 8/100\n","83/83 [==============================] - 87s 1s/step - loss: 0.0019 - val_loss: 0.0019\n","Epoch 9/100\n","83/83 [==============================] - 84s 1s/step - loss: 0.0019 - val_loss: 0.0019\n","Epoch 10/100\n","83/83 [==============================] - 84s 1s/step - loss: 0.0019 - val_loss: 0.0019\n","Epoch 11/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0019 - val_loss: 0.0019\n","Epoch 12/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0019 - val_loss: 0.0019\n","Epoch 13/100\n","83/83 [==============================] - 87s 1s/step - loss: 0.0018 - val_loss: 0.0019\n","Epoch 14/100\n","83/83 [==============================] - 87s 1s/step - loss: 0.0018 - val_loss: 0.0019\n","Epoch 15/100\n","83/83 [==============================] - 84s 1s/step - loss: 0.0018 - val_loss: 0.0019\n","Epoch 16/100\n","83/83 [==============================] - 81s 980ms/step - loss: 0.0018 - val_loss: 0.0019\n","Epoch 17/100\n","83/83 [==============================] - 83s 1s/step - loss: 0.0018 - val_loss: 0.0019\n","Epoch 18/100\n","83/83 [==============================] - 85s 1s/step - loss: 0.0018 - val_loss: 0.0019\n","Epoch 19/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0018 - val_loss: 0.0019\n","Epoch 20/100\n","83/83 [==============================] - 85s 1s/step - loss: 0.0018 - val_loss: 0.0019\n","Epoch 21/100\n","83/83 [==============================] - 88s 1s/step - loss: 0.0017 - val_loss: 0.0019\n","Epoch 22/100\n","83/83 [==============================] - 87s 1s/step - loss: 0.0017 - val_loss: 0.0019\n","Epoch 23/100\n","83/83 [==============================] - 82s 986ms/step - loss: 0.0017 - val_loss: 0.0019\n","Epoch 24/100\n","83/83 [==============================] - 85s 1s/step - loss: 0.0017 - val_loss: 0.0019\n","Epoch 25/100\n","83/83 [==============================] - 87s 1s/step - loss: 0.0017 - val_loss: 0.0019\n","Epoch 26/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0016 - val_loss: 0.0020\n","Epoch 27/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0016 - val_loss: 0.0020\n","Epoch 28/100\n","83/83 [==============================] - 82s 988ms/step - loss: 0.0016 - val_loss: 0.0021\n","Epoch 29/100\n","83/83 [==============================] - 85s 1s/step - loss: 0.0016 - val_loss: 0.0020\n","Epoch 30/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0016 - val_loss: 0.0021\n","Epoch 31/100\n","83/83 [==============================] - 87s 1s/step - loss: 0.0015 - val_loss: 0.0020\n","Epoch 32/100\n","83/83 [==============================] - 87s 1s/step - loss: 0.0015 - val_loss: 0.0021\n","Epoch 33/100\n","83/83 [==============================] - 81s 980ms/step - loss: 0.0015 - val_loss: 0.0021\n","Epoch 34/100\n","83/83 [==============================] - 81s 982ms/step - loss: 0.0015 - val_loss: 0.0022\n","Epoch 35/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0015 - val_loss: 0.0022\n","Epoch 36/100\n","83/83 [==============================] - 85s 1s/step - loss: 0.0015 - val_loss: 0.0021\n","Epoch 37/100\n","83/83 [==============================] - 88s 1s/step - loss: 0.0015 - val_loss: 0.0022\n","Epoch 38/100\n","83/83 [==============================] - 84s 1s/step - loss: 0.0015 - val_loss: 0.0023\n","Epoch 39/100\n","83/83 [==============================] - 83s 1s/step - loss: 0.0014 - val_loss: 0.0023\n","Epoch 40/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0014 - val_loss: 0.0024\n","Epoch 41/100\n","83/83 [==============================] - 86s 1s/step - loss: 0.0014 - val_loss: 0.0024\n","Epoch 42/100\n","83/83 [==============================] - 87s 1s/step - loss: 0.0014 - val_loss: 0.0024\n","Epoch 43/100\n","83/83 [==============================] - 82s 983ms/step - loss: 0.0014 - val_loss: 0.0025\n","Epoch 44/100\n","83/83 [==============================] - 80s 963ms/step - loss: 0.0014 - val_loss: 0.0025\n","Epoch 45/100\n","83/83 [==============================] - 79s 952ms/step - loss: 0.0014 - val_loss: 0.0025\n","Epoch 46/100\n","83/83 [==============================] - 78s 943ms/step - loss: 0.0014 - val_loss: 0.0025\n","Epoch 47/100\n","83/83 [==============================] - 78s 941ms/step - loss: 0.0014 - val_loss: 0.0028\n","Epoch 48/100\n","83/83 [==============================] - 79s 946ms/step - loss: 0.0014 - val_loss: 0.0026\n","Epoch 49/100\n","83/83 [==============================] - 78s 939ms/step - loss: 0.0014 - val_loss: 0.0028\n","Epoch 50/100\n","83/83 [==============================] - 78s 936ms/step - loss: 0.0014 - val_loss: 0.0029\n","Epoch 51/100\n","83/83 [==============================] - 77s 931ms/step - loss: 0.0014 - val_loss: 0.0030\n","Epoch 52/100\n","83/83 [==============================] - 77s 928ms/step - loss: 0.0014 - val_loss: 0.0028\n","Epoch 53/100\n","83/83 [==============================] - 78s 935ms/step - loss: 0.0014 - val_loss: 0.0028\n","Epoch 54/100\n","83/83 [==============================] - 77s 930ms/step - loss: 0.0014 - val_loss: 0.0030\n","Epoch 55/100\n","83/83 [==============================] - 77s 932ms/step - loss: 0.0014 - val_loss: 0.0030\n","Epoch 56/100\n","83/83 [==============================] - 77s 930ms/step - loss: 0.0014 - val_loss: 0.0030\n","Epoch 57/100\n","83/83 [==============================] - 77s 932ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 58/100\n","83/83 [==============================] - 77s 929ms/step - loss: 0.0013 - val_loss: 0.0027\n","Epoch 59/100\n","83/83 [==============================] - 77s 932ms/step - loss: 0.0013 - val_loss: 0.0031\n","Epoch 60/100\n","83/83 [==============================] - 77s 925ms/step - loss: 0.0013 - val_loss: 0.0030\n","Epoch 61/100\n","83/83 [==============================] - 77s 931ms/step - loss: 0.0013 - val_loss: 0.0031\n","Epoch 62/100\n","83/83 [==============================] - 78s 934ms/step - loss: 0.0013 - val_loss: 0.0030\n","Epoch 63/100\n","83/83 [==============================] - 78s 936ms/step - loss: 0.0013 - val_loss: 0.0033\n","Epoch 64/100\n","83/83 [==============================] - 77s 931ms/step - loss: 0.0013 - val_loss: 0.0029\n","Epoch 65/100\n","83/83 [==============================] - 77s 933ms/step - loss: 0.0013 - val_loss: 0.0031\n","Epoch 66/100\n","83/83 [==============================] - 77s 930ms/step - loss: 0.0013 - val_loss: 0.0033\n","Epoch 67/100\n","83/83 [==============================] - 77s 930ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 68/100\n","83/83 [==============================] - 77s 932ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 69/100\n","83/83 [==============================] - 79s 951ms/step - loss: 0.0013 - val_loss: 0.0036\n","Epoch 70/100\n","83/83 [==============================] - 79s 949ms/step - loss: 0.0013 - val_loss: 0.0029\n","Epoch 71/100\n","83/83 [==============================] - 78s 942ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 72/100\n","83/83 [==============================] - 78s 939ms/step - loss: 0.0013 - val_loss: 0.0030\n","Epoch 73/100\n","83/83 [==============================] - 78s 941ms/step - loss: 0.0013 - val_loss: 0.0031\n","Epoch 74/100\n","83/83 [==============================] - 78s 935ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 75/100\n","83/83 [==============================] - 77s 932ms/step - loss: 0.0013 - val_loss: 0.0034\n","Epoch 76/100\n","83/83 [==============================] - 77s 932ms/step - loss: 0.0013 - val_loss: 0.0033\n","Epoch 77/100\n","83/83 [==============================] - 78s 935ms/step - loss: 0.0013 - val_loss: 0.0035\n","Epoch 78/100\n","83/83 [==============================] - 77s 932ms/step - loss: 0.0013 - val_loss: 0.0033\n","Epoch 79/100\n","83/83 [==============================] - 77s 932ms/step - loss: 0.0013 - val_loss: 0.0033\n","Epoch 80/100\n","83/83 [==============================] - 77s 930ms/step - loss: 0.0013 - val_loss: 0.0034\n","Epoch 81/100\n","83/83 [==============================] - 77s 933ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 82/100\n","83/83 [==============================] - 77s 929ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 83/100\n","83/83 [==============================] - 78s 935ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 84/100\n","83/83 [==============================] - 79s 953ms/step - loss: 0.0013 - val_loss: 0.0035\n","Epoch 85/100\n","83/83 [==============================] - 79s 956ms/step - loss: 0.0013 - val_loss: 0.0036\n","Epoch 86/100\n","83/83 [==============================] - 79s 951ms/step - loss: 0.0013 - val_loss: 0.0034\n","Epoch 87/100\n","83/83 [==============================] - 79s 947ms/step - loss: 0.0013 - val_loss: 0.0035\n","Epoch 88/100\n","83/83 [==============================] - 79s 952ms/step - loss: 0.0013 - val_loss: 0.0034\n","Epoch 89/100\n","83/83 [==============================] - 79s 947ms/step - loss: 0.0013 - val_loss: 0.0031\n","Epoch 90/100\n","83/83 [==============================] - 79s 949ms/step - loss: 0.0012 - val_loss: 0.0031\n","Epoch 91/100\n","83/83 [==============================] - 78s 944ms/step - loss: 0.0013 - val_loss: 0.0029\n","Epoch 92/100\n","83/83 [==============================] - 79s 950ms/step - loss: 0.0013 - val_loss: 0.0031\n","Epoch 93/100\n","83/83 [==============================] - 79s 947ms/step - loss: 0.0013 - val_loss: 0.0031\n","Epoch 94/100\n","83/83 [==============================] - 79s 950ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 95/100\n","83/83 [==============================] - 80s 959ms/step - loss: 0.0013 - val_loss: 0.0033\n","Epoch 96/100\n","83/83 [==============================] - 79s 958ms/step - loss: 0.0013 - val_loss: 0.0032\n","Epoch 97/100\n","83/83 [==============================] - 80s 964ms/step - loss: 0.0013 - val_loss: 0.0031\n","Epoch 98/100\n","83/83 [==============================] - 80s 968ms/step - loss: 0.0012 - val_loss: 0.0031\n","Epoch 99/100\n","83/83 [==============================] - 79s 948ms/step - loss: 0.0012 - val_loss: 0.0030\n","Epoch 100/100\n","83/83 [==============================] - 79s 956ms/step - loss: 0.0012 - val_loss: 0.0030\n"],"name":"stdout"}]},{"metadata":{"id":"bQ5OP0PUw12E","colab_type":"code","colab":{}},"cell_type":"code","source":["# Saving the trained model\n","\n","v2_model.save('/gdrive/My Drive/FaceRecognition/models/mobile-net/mobilenetv2_v1.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MlFJQFVdGhep","colab_type":"text"},"cell_type":"markdown","source":["### MobileNet V1"]},{"metadata":{"id":"54y3OlGDQ41r","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rates = [0.05, 0.03, 0.01, 0.005, 0.001]\n","\n","batch_size = 128\n","num_epochs = 5\n","queue_size = 12\n","\n","Wsave = v1_model.get_weights()\n","for idx, learning_rate in enumerate(learning_rates):\n","  print('[ITERATION {}] Learning rate: {}'.format(idx+1, learning_rate))\n","  v1_model.compile(optimizer=keras.optimizers.Nadam(lr=learning_rate),\n","                  loss=losses.mean_squared_error,\n","                   metrics=[losses.mean_absolute_error])\n","  \n","  v1_model.fit_generator(generator=training_batch_generator,\n","                       steps_per_epoch=(training_size // batch_size),\n","                       epochs=num_epochs,\n","                       verbose=1,\n","                       validation_data=val_batch_generator,\n","                       validation_steps=(val_size // batch_size),\n","                       max_queue_size=queue_size,\n","                       workers=10)\n","  print('Setting the weights back... \\n\\n')\n","  v1_model.set_weights(Wsave)\n","  \n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"rhG_A8_tGjQ0","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import losses\n","\n","batch_size = 128\n","num_epochs = 70\n","queue_size = 12\n","\n","v1_model.compile(optimizer=keras.optimizers.Nadam(lr=0.001),\n","                   loss=losses.mean_squared_error,\n","                   metrics=[losses.cosine_proximity,\n","                           losses.mean_absolute_error])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3fFHOl3gGmEu","colab_type":"code","outputId":"bd0c99f0-9869-4470-c104-5a02363bca5a","executionInfo":{"status":"ok","timestamp":1539924229127,"user_tz":180,"elapsed":5838841,"user":{"displayName":"Pedro Prates","photoUrl":"","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":2434}},"cell_type":"code","source":["v1_model.fit_generator(generator=training_batch_generator,\n","                       epochs=num_epochs,\n","                       verbose=1,\n","                       validation_data=val_batch_generator,\n","                       validation_steps=(val_size // batch_size),\n","                       max_queue_size=queue_size,\n","                       workers=10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/70\n","83/83 [==============================] - 99s 1s/step - loss: 0.0259 - cosine_proximity: -0.0141 - mean_absolute_error: 0.1065 - val_loss: 0.0039 - val_cosine_proximity: 0.0236 - val_mean_absolute_error: 0.0498\n","Epoch 2/70\n","83/83 [==============================] - 90s 1s/step - loss: 0.0020 - cosine_proximity: -0.1228 - mean_absolute_error: 0.0361 - val_loss: 0.0027 - val_cosine_proximity: 1.8022e-05 - val_mean_absolute_error: 0.0406\n","Epoch 3/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0020 - cosine_proximity: -0.1354 - mean_absolute_error: 0.0360 - val_loss: 0.0020 - val_cosine_proximity: -0.0474 - val_mean_absolute_error: 0.0360\n","Epoch 4/70\n","83/83 [==============================] - 82s 986ms/step - loss: 0.0019 - cosine_proximity: -0.1538 - mean_absolute_error: 0.0350 - val_loss: 0.0019 - val_cosine_proximity: -0.1632 - val_mean_absolute_error: 0.0350\n","Epoch 5/70\n","83/83 [==============================] - 83s 1000ms/step - loss: 0.0019 - cosine_proximity: -0.1573 - mean_absolute_error: 0.0350 - val_loss: 0.0019 - val_cosine_proximity: -0.1657 - val_mean_absolute_error: 0.0349\n","Epoch 6/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0019 - cosine_proximity: -0.1659 - mean_absolute_error: 0.0350 - val_loss: 0.0031 - val_cosine_proximity: -0.0924 - val_mean_absolute_error: 0.0424\n","Epoch 7/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0019 - cosine_proximity: -0.1606 - mean_absolute_error: 0.0349 - val_loss: 0.0019 - val_cosine_proximity: -0.1641 - val_mean_absolute_error: 0.0350\n","Epoch 8/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0019 - cosine_proximity: -0.1785 - mean_absolute_error: 0.0348 - val_loss: 0.0019 - val_cosine_proximity: -0.1809 - val_mean_absolute_error: 0.0348\n","Epoch 9/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0019 - cosine_proximity: -0.1947 - mean_absolute_error: 0.0347 - val_loss: 0.0019 - val_cosine_proximity: -0.1999 - val_mean_absolute_error: 0.0347\n","Epoch 10/70\n","83/83 [==============================] - 82s 987ms/step - loss: 0.0019 - cosine_proximity: -0.2129 - mean_absolute_error: 0.0345 - val_loss: 0.0019 - val_cosine_proximity: -0.2043 - val_mean_absolute_error: 0.0346\n","Epoch 11/70\n","83/83 [==============================] - 71s 853ms/step - loss: 0.0018 - cosine_proximity: -0.2359 - mean_absolute_error: 0.0342 - val_loss: 0.0018 - val_cosine_proximity: -0.2306 - val_mean_absolute_error: 0.0344\n","Epoch 12/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0018 - cosine_proximity: -0.2575 - mean_absolute_error: 0.0340 - val_loss: 0.0019 - val_cosine_proximity: -0.2213 - val_mean_absolute_error: 0.0346\n","Epoch 13/70\n","83/83 [==============================] - 90s 1s/step - loss: 0.0018 - cosine_proximity: -0.2766 - mean_absolute_error: 0.0338 - val_loss: 0.0018 - val_cosine_proximity: -0.2646 - val_mean_absolute_error: 0.0340\n","Epoch 14/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0018 - cosine_proximity: -0.2944 - mean_absolute_error: 0.0336 - val_loss: 0.0018 - val_cosine_proximity: -0.2500 - val_mean_absolute_error: 0.0342\n","Epoch 15/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0017 - cosine_proximity: -0.3110 - mean_absolute_error: 0.0334 - val_loss: 0.0030 - val_cosine_proximity: -0.1858 - val_mean_absolute_error: 0.0425\n","Epoch 16/70\n","83/83 [==============================] - 82s 991ms/step - loss: 0.0017 - cosine_proximity: -0.3282 - mean_absolute_error: 0.0331 - val_loss: 0.0018 - val_cosine_proximity: -0.2831 - val_mean_absolute_error: 0.0339\n","Epoch 17/70\n","83/83 [==============================] - 83s 998ms/step - loss: 0.0017 - cosine_proximity: -0.3438 - mean_absolute_error: 0.0329 - val_loss: 0.0018 - val_cosine_proximity: -0.2839 - val_mean_absolute_error: 0.0340\n","Epoch 18/70\n","83/83 [==============================] - 83s 998ms/step - loss: 0.0017 - cosine_proximity: -0.3623 - mean_absolute_error: 0.0326 - val_loss: 0.0018 - val_cosine_proximity: -0.3041 - val_mean_absolute_error: 0.0337\n","Epoch 19/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0017 - cosine_proximity: -0.3810 - mean_absolute_error: 0.0323 - val_loss: 0.0018 - val_cosine_proximity: -0.3121 - val_mean_absolute_error: 0.0337\n","Epoch 20/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0016 - cosine_proximity: -0.3991 - mean_absolute_error: 0.0321 - val_loss: 0.0018 - val_cosine_proximity: -0.3161 - val_mean_absolute_error: 0.0338\n","Epoch 21/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0016 - cosine_proximity: -0.4168 - mean_absolute_error: 0.0317 - val_loss: 0.0018 - val_cosine_proximity: -0.3084 - val_mean_absolute_error: 0.0341\n","Epoch 22/70\n","83/83 [==============================] - 70s 841ms/step - loss: 0.0017 - cosine_proximity: -0.3909 - mean_absolute_error: 0.0324 - val_loss: 0.0020 - val_cosine_proximity: -0.1059 - val_mean_absolute_error: 0.0360\n","Epoch 23/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0016 - cosine_proximity: -0.4053 - mean_absolute_error: 0.0320 - val_loss: 0.0018 - val_cosine_proximity: -0.2989 - val_mean_absolute_error: 0.0338\n","Epoch 24/70\n","83/83 [==============================] - 89s 1s/step - loss: 0.0016 - cosine_proximity: -0.4416 - mean_absolute_error: 0.0312 - val_loss: 0.0017 - val_cosine_proximity: -0.3380 - val_mean_absolute_error: 0.0333\n","Epoch 25/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0015 - cosine_proximity: -0.4636 - mean_absolute_error: 0.0308 - val_loss: 0.0017 - val_cosine_proximity: -0.3750 - val_mean_absolute_error: 0.0328\n","Epoch 26/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0015 - cosine_proximity: -0.4805 - mean_absolute_error: 0.0304 - val_loss: 0.0018 - val_cosine_proximity: -0.3375 - val_mean_absolute_error: 0.0334\n","Epoch 27/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0015 - cosine_proximity: -0.4950 - mean_absolute_error: 0.0301 - val_loss: 0.0018 - val_cosine_proximity: -0.3509 - val_mean_absolute_error: 0.0335\n","Epoch 28/70\n","83/83 [==============================] - 82s 989ms/step - loss: 0.0014 - cosine_proximity: -0.5119 - mean_absolute_error: 0.0296 - val_loss: 0.0017 - val_cosine_proximity: -0.3861 - val_mean_absolute_error: 0.0327\n","Epoch 29/70\n","83/83 [==============================] - 82s 992ms/step - loss: 0.0014 - cosine_proximity: -0.5256 - mean_absolute_error: 0.0292 - val_loss: 0.0018 - val_cosine_proximity: -0.3717 - val_mean_absolute_error: 0.0334\n","Epoch 30/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0014 - cosine_proximity: -0.5387 - mean_absolute_error: 0.0289 - val_loss: 0.0018 - val_cosine_proximity: -0.3555 - val_mean_absolute_error: 0.0342\n","Epoch 31/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0014 - cosine_proximity: -0.5505 - mean_absolute_error: 0.0285 - val_loss: 0.0018 - val_cosine_proximity: -0.3639 - val_mean_absolute_error: 0.0336\n","Epoch 32/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0013 - cosine_proximity: -0.5616 - mean_absolute_error: 0.0281 - val_loss: 0.0017 - val_cosine_proximity: -0.3850 - val_mean_absolute_error: 0.0332\n","Epoch 33/70\n","83/83 [==============================] - 73s 875ms/step - loss: 0.0013 - cosine_proximity: -0.5723 - mean_absolute_error: 0.0278 - val_loss: 0.0019 - val_cosine_proximity: -0.3394 - val_mean_absolute_error: 0.0345\n","Epoch 34/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0013 - cosine_proximity: -0.5812 - mean_absolute_error: 0.0274 - val_loss: 0.0018 - val_cosine_proximity: -0.3872 - val_mean_absolute_error: 0.0335\n","Epoch 35/70\n","83/83 [==============================] - 88s 1s/step - loss: 0.0013 - cosine_proximity: -0.5897 - mean_absolute_error: 0.0271 - val_loss: 0.0018 - val_cosine_proximity: -0.3782 - val_mean_absolute_error: 0.0340\n","Epoch 36/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0013 - cosine_proximity: -0.5965 - mean_absolute_error: 0.0269 - val_loss: 0.0018 - val_cosine_proximity: -0.3618 - val_mean_absolute_error: 0.0341\n","Epoch 37/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0012 - cosine_proximity: -0.6003 - mean_absolute_error: 0.0267 - val_loss: 0.0017 - val_cosine_proximity: -0.4057 - val_mean_absolute_error: 0.0327\n","Epoch 38/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0012 - cosine_proximity: -0.6080 - mean_absolute_error: 0.0264 - val_loss: 0.0019 - val_cosine_proximity: -0.3610 - val_mean_absolute_error: 0.0345\n","Epoch 39/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0012 - cosine_proximity: -0.6146 - mean_absolute_error: 0.0261 - val_loss: 0.0019 - val_cosine_proximity: -0.3400 - val_mean_absolute_error: 0.0348\n","Epoch 40/70\n","83/83 [==============================] - 82s 994ms/step - loss: 0.0012 - cosine_proximity: -0.6213 - mean_absolute_error: 0.0257 - val_loss: 0.0019 - val_cosine_proximity: -0.3637 - val_mean_absolute_error: 0.0342\n","Epoch 41/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0012 - cosine_proximity: -0.6249 - mean_absolute_error: 0.0256 - val_loss: 0.0017 - val_cosine_proximity: -0.3970 - val_mean_absolute_error: 0.0331\n","Epoch 42/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0012 - cosine_proximity: -0.6295 - mean_absolute_error: 0.0253 - val_loss: 0.0019 - val_cosine_proximity: -0.3705 - val_mean_absolute_error: 0.0343\n","Epoch 43/70\n","83/83 [==============================] - 83s 994ms/step - loss: 0.0012 - cosine_proximity: -0.6320 - mean_absolute_error: 0.0252 - val_loss: 0.0017 - val_cosine_proximity: -0.3857 - val_mean_absolute_error: 0.0331\n","Epoch 44/70\n","83/83 [==============================] - 73s 881ms/step - loss: 0.0012 - cosine_proximity: -0.6364 - mean_absolute_error: 0.0250 - val_loss: 0.0018 - val_cosine_proximity: -0.3861 - val_mean_absolute_error: 0.0338\n","Epoch 45/70\n","83/83 [==============================] - 86s 1s/step - loss: 0.0011 - cosine_proximity: -0.6409 - mean_absolute_error: 0.0247 - val_loss: 0.0018 - val_cosine_proximity: -0.3759 - val_mean_absolute_error: 0.0337\n","Epoch 46/70\n","83/83 [==============================] - 89s 1s/step - loss: 0.0011 - cosine_proximity: -0.6439 - mean_absolute_error: 0.0245 - val_loss: 0.0017 - val_cosine_proximity: -0.3935 - val_mean_absolute_error: 0.0332\n","Epoch 47/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0011 - cosine_proximity: -0.6465 - mean_absolute_error: 0.0244 - val_loss: 0.0018 - val_cosine_proximity: -0.3798 - val_mean_absolute_error: 0.0337\n","Epoch 48/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0011 - cosine_proximity: -0.6481 - mean_absolute_error: 0.0243 - val_loss: 0.0017 - val_cosine_proximity: -0.3905 - val_mean_absolute_error: 0.0331\n","Epoch 49/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0011 - cosine_proximity: -0.6514 - mean_absolute_error: 0.0240 - val_loss: 0.0018 - val_cosine_proximity: -0.3883 - val_mean_absolute_error: 0.0335\n","Epoch 50/70\n","83/83 [==============================] - 83s 997ms/step - loss: 0.0011 - cosine_proximity: -0.6540 - mean_absolute_error: 0.0238 - val_loss: 0.0018 - val_cosine_proximity: -0.3931 - val_mean_absolute_error: 0.0334\n","Epoch 51/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0011 - cosine_proximity: -0.6553 - mean_absolute_error: 0.0238 - val_loss: 0.0018 - val_cosine_proximity: -0.3897 - val_mean_absolute_error: 0.0338\n","Epoch 52/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0011 - cosine_proximity: -0.6576 - mean_absolute_error: 0.0236 - val_loss: 0.0017 - val_cosine_proximity: -0.4114 - val_mean_absolute_error: 0.0327\n","Epoch 53/70\n","83/83 [==============================] - 83s 994ms/step - loss: 0.0011 - cosine_proximity: -0.6589 - mean_absolute_error: 0.0235 - val_loss: 0.0017 - val_cosine_proximity: -0.3967 - val_mean_absolute_error: 0.0332\n","Epoch 54/70\n","83/83 [==============================] - 82s 991ms/step - loss: 0.0011 - cosine_proximity: -0.6605 - mean_absolute_error: 0.0234 - val_loss: 0.0017 - val_cosine_proximity: -0.4123 - val_mean_absolute_error: 0.0327\n","Epoch 55/70\n","83/83 [==============================] - 71s 856ms/step - loss: 0.0011 - cosine_proximity: -0.6623 - mean_absolute_error: 0.0232 - val_loss: 0.0017 - val_cosine_proximity: -0.4111 - val_mean_absolute_error: 0.0326\n","Epoch 56/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0011 - cosine_proximity: -0.6637 - mean_absolute_error: 0.0231 - val_loss: 0.0017 - val_cosine_proximity: -0.4184 - val_mean_absolute_error: 0.0326\n","Epoch 57/70\n","83/83 [==============================] - 90s 1s/step - loss: 0.0011 - cosine_proximity: -0.6633 - mean_absolute_error: 0.0232 - val_loss: 0.0017 - val_cosine_proximity: -0.3971 - val_mean_absolute_error: 0.0332\n","Epoch 58/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0011 - cosine_proximity: -0.6634 - mean_absolute_error: 0.0232 - val_loss: 0.0018 - val_cosine_proximity: -0.3912 - val_mean_absolute_error: 0.0338\n","Epoch 59/70\n","83/83 [==============================] - 84s 1s/step - loss: 0.0011 - cosine_proximity: -0.6640 - mean_absolute_error: 0.0231 - val_loss: 0.0017 - val_cosine_proximity: -0.3955 - val_mean_absolute_error: 0.0332\n","Epoch 60/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0011 - cosine_proximity: -0.6664 - mean_absolute_error: 0.0229 - val_loss: 0.0017 - val_cosine_proximity: -0.4064 - val_mean_absolute_error: 0.0330\n","Epoch 61/70\n","83/83 [==============================] - 82s 988ms/step - loss: 0.0011 - cosine_proximity: -0.6677 - mean_absolute_error: 0.0228 - val_loss: 0.0017 - val_cosine_proximity: -0.4008 - val_mean_absolute_error: 0.0328\n","Epoch 62/70\n","83/83 [==============================] - 82s 994ms/step - loss: 0.0011 - cosine_proximity: -0.6681 - mean_absolute_error: 0.0227 - val_loss: 0.0017 - val_cosine_proximity: -0.4051 - val_mean_absolute_error: 0.0327\n","Epoch 63/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0011 - cosine_proximity: -0.6688 - mean_absolute_error: 0.0227 - val_loss: 0.0017 - val_cosine_proximity: -0.4030 - val_mean_absolute_error: 0.0327\n","Epoch 64/70\n","83/83 [==============================] - 82s 993ms/step - loss: 0.0011 - cosine_proximity: -0.6694 - mean_absolute_error: 0.0226 - val_loss: 0.0017 - val_cosine_proximity: -0.4094 - val_mean_absolute_error: 0.0327\n","Epoch 65/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0011 - cosine_proximity: -0.6694 - mean_absolute_error: 0.0226 - val_loss: 0.0017 - val_cosine_proximity: -0.4099 - val_mean_absolute_error: 0.0330\n","Epoch 66/70\n","83/83 [==============================] - 73s 878ms/step - loss: 0.0011 - cosine_proximity: -0.6705 - mean_absolute_error: 0.0225 - val_loss: 0.0017 - val_cosine_proximity: -0.3951 - val_mean_absolute_error: 0.0327\n","Epoch 67/70\n","83/83 [==============================] - 85s 1s/step - loss: 0.0011 - cosine_proximity: -0.6700 - mean_absolute_error: 0.0226 - val_loss: 0.0017 - val_cosine_proximity: -0.4027 - val_mean_absolute_error: 0.0331\n","Epoch 68/70\n","83/83 [==============================] - 87s 1s/step - loss: 0.0011 - cosine_proximity: -0.6704 - mean_absolute_error: 0.0225 - val_loss: 0.0017 - val_cosine_proximity: -0.4101 - val_mean_absolute_error: 0.0326\n","Epoch 69/70\n","83/83 [==============================] - 83s 997ms/step - loss: 0.0011 - cosine_proximity: -0.6716 - mean_absolute_error: 0.0224 - val_loss: 0.0017 - val_cosine_proximity: -0.4028 - val_mean_absolute_error: 0.0330\n","Epoch 70/70\n","83/83 [==============================] - 83s 1s/step - loss: 0.0011 - cosine_proximity: -0.6721 - mean_absolute_error: 0.0223 - val_loss: 0.0018 - val_cosine_proximity: -0.3798 - val_mean_absolute_error: 0.0335\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f88948c3f28>"]},"metadata":{"tags":[]},"execution_count":25}]},{"metadata":{"id":"W1-gVV0kSYGz","colab_type":"code","colab":{}},"cell_type":"code","source":["# Saving the trained model\n","\n","v1_model.save('/gdrive/My Drive/FaceRecognition/models/mobile-net/mobilenetv1_v1.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IO6BgC811vyW","colab_type":"text"},"cell_type":"markdown","source":["# Validating\n","\n","We have 1324 images on the test set. First we need to identify each individual and then run forward propagation on our model on all of our images. And then, using our test set, we will check, based on our known distance function (calculates an euclidean distance or cosine similarity between two embeddings), which predictions our model is capable of doing. "]},{"metadata":{"id":"xB_xuGTm5HE0","colab_type":"code","colab":{}},"cell_type":"code","source":["# Saving the X_test used for the model_v1 and model_v2 h5 files.\n","np.save('/gdrive/My Drive/FaceRecognition/datasets/lfw/xtest.npy', X_test)\n","np.save('/gdrive/My Drive/FaceRecognition/datasets/lfw/ytest.npy', y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EO-_wzdr8aqH","colab_type":"text"},"cell_type":"markdown","source":["## Running forward propagation\n","\n","Running forward propagation on all images. The images will be saved as `output_v/` where *v* is the version of its model. The desired shape on each embedding is `(1, 512)`"]},{"metadata":{"id":"-5vmzqcYuRoa","colab_type":"text"},"cell_type":"markdown","source":["## Mobilenetv2 V1\n","\n","Running on MobilenetV2 trained with learning rate 0.001 and for 100 epochs."]},{"metadata":{"id":"Ttk2_D8_AHfR","colab_type":"code","colab":{}},"cell_type":"code","source":["model = keras.models.load_model('/gdrive/My Drive/FaceRecognition/models/mobile-net/mobilenetv2_v1.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RtW0ksbq-gT7","colab_type":"code","outputId":"d186270d-f3b1-41a7-cffc-e57dea6cade8","executionInfo":{"status":"ok","timestamp":1539914939341,"user_tz":180,"elapsed":1243555,"user":{"displayName":"Pedro Prates","photoUrl":"","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["base_path = '/gdrive/My Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160/'\n","list_folders = os.listdir(base_path)\n","list_folders = [os.path.join(base_path, x) for x in list_folders]\n","\n","for folder in progressbar.progressbar(list_folders):\n","  if not os.path.isdir(folder):\n","    continue\n","    \n","  list_images = os.listdir(folder)\n","  list_images = [os.path.join(folder, image) for image in list_images]\n","  list_images = list(filter(lambda x: os.path.isfile(x), list_images))\n","  filenames = [x.split('/')[-1].split('.')[0] for x in list_images]\n","  output_filenames = [x + '.npy' for x in filenames]\n","  output_folder = os.path.join(folder, 'mobilenetv2_v1')\n","  \n","  # Get the embeddings\n","  images = np.array([misc.imread(f) for f in list_images])\n","  embeddings = model.predict(images)\n","  \n","  if not os.path.exists(os.path.join(base_path, output_folder)):\n","    os.makedirs(os.path.join(base_path, output_folder))\n","  for idx, embedding in enumerate(embeddings):\n","    emb_to_save = embedding.reshape(1, *embedding.shape)\n","    np.save(os.path.join(output_folder, output_filenames[idx]), emb_to_save)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100% (5754 of 5754) |####################| Elapsed Time: 0:20:42 Time:  0:20:42\n"],"name":"stderr"}]},{"metadata":{"id":"GElBaCNjuaXR","colab_type":"text"},"cell_type":"markdown","source":["## Mobilenetv1 V1\n","\n","Running on MobilenetV1 trained with learning rate 0.001 and for 70 epochs"]},{"metadata":{"id":"hHJiX3gkuiJI","colab_type":"code","colab":{}},"cell_type":"code","source":["model = keras.models.load_model('/gdrive/My Drive/FaceRecognition/models/mobile-net/mobilenetv1_v1.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4UhVJ03XukFI","colab_type":"code","outputId":"29e64b57-b7c9-4f2b-a428-486265259294","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["base_path = '/gdrive/My Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160/'\n","list_folders = os.listdir(base_path)\n","list_folders = [os.path.join(base_path, x) for x in list_folders]\n","\n","for folder in progressbar.progressbar(list_folders):\n","  if not os.path.isdir(folder):\n","    continue\n","    \n","  list_images = os.listdir(folder)\n","  list_images = [os.path.join(folder, image) for image in list_images]\n","  list_images = list(filter(lambda x: os.path.isfile(x), list_images))\n","  filenames = [x.split('/')[-1].split('.')[0] for x in list_images]\n","  output_filenames = [x + '.npy' for x in filenames]\n","  output_folder = os.path.join(folder, 'mobilenetv1_v1')\n","  \n","  # Get the embeddings\n","  images = np.array([misc.imread(f) for f in list_images])\n","  embeddings = model.predict(images)\n","  \n","  if not os.path.exists(os.path.join(base_path, output_folder)):\n","    os.makedirs(os.path.join(base_path, output_folder))\n","  for idx, embedding in enumerate(embeddings):\n","    emb_to_save = embedding.reshape(1, *embedding.shape)\n","    np.save(os.path.join(output_folder, output_filenames[idx]), emb_to_save)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  0% (9 of 5754) |                       | Elapsed Time: 0:00:00 ETA:   0:09:20"],"name":"stderr"}]},{"metadata":{"id":"vFrTEH3e0Zud","colab_type":"code","outputId":"490a90cd-12c0-4612-f133-8ceeec7e04ec","executionInfo":{"status":"ok","timestamp":1539944078992,"user_tz":180,"elapsed":1045,"user":{"displayName":"Pedro Prates","photoUrl":"","userId":"18091520408936459745"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["model"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.engine.training.Model at 0x7fdb545547f0>"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"N8x_EQtPErpH","colab_type":"text"},"cell_type":"markdown","source":["## Running Validation"]},{"metadata":{"id":"lAg1UI5TL3Rd","colab_type":"code","colab":{}},"cell_type":"code","source":["# Loading the dataset\n","dataset = np.load('/gdrive/My Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160/embeddings_test_mac.npy')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gw0qW_6b2Sdl","colab_type":"code","colab":{}},"cell_type":"code","source":["def distance(embeddings1, embeddings2, distance_metric='euclidean'):\n","    \"\"\" Calculate the distance between two embeddings. Currently working with euclidean and cosine similarity. \n","\n","        :param embeddings1: First embedding\n","        :param embeddings2: Second embedding\n","        :param distance_metric: Distance metric to be used to make the calculation. Should be either: 'euclidean' or 'cosine'\n","\n","        :returns: The distance between the `embeddings1` and `embeddings2`\n","    \"\"\"\n","    assert distance_metric in ['euclidean', 'cosine'], \"The distance metric should be either 'euclidean' or 'cosine'\"\n","\n","    if distance_metric == 'euclidean':\n","        diff = np.subtract(embeddings1, embeddings2)\n","        dist = np.sum(np.square(diff), 1)\n","\n","    elif distance_metric == 'cosine':\n","        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n","        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n","        similarity = dot / norm\n","        dist = np.arccos(similarity) / math.pi\n","\n","    return dist"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UAMlj3mn-mjG","colab_type":"code","colab":{}},"cell_type":"code","source":["def only_alpha(string):\n","  return all(not a.isdigit() for a in string)\n","\n","def clean_name(path_name, with_number=False):\n","  path_name = path_name.split('/')[-1]\n","  path_name = path_name.split('.')[0]\n","  \n","  if with_number:\n","    return path_name\n","  \n","  list_names = path_name.split('_')\n","  \n","  list_names = list(filter(lambda x: only_alpha(x), list_names))\n","  name = '_'.join(list_names)\n","  return name"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dupp55YX7DZH","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_names(data):\n","  \"\"\" Return the list of unique names that compose the dataset\n","  \n","      :params data: The dataset to be analyzed\n","  \"\"\"\n","  names = []\n","  for image_dict in data:\n","    name = clean_name(image_dict['name'])\n","    \n","    if name not in names:\n","      names.append(name)\n","      \n","  idxs = np.arange(len(names))\n","  name_to_idx = dict(zip(iter(names), iter(idxs)))\n","  return np.array(names), name_to_idx"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WWqzcvNZC45q","colab_type":"code","colab":{}},"cell_type":"code","source":["def is_diff(x, y):\n","  return x.split('/')[-1].split('.')[0] != y.split('/')[-1].split('.')[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RQ5qV_43VGeN","colab_type":"code","colab":{}},"cell_type":"code","source":["def predict_face(image_path,\n","                embedding,\n","                dataset,\n","                threshold=.1,\n","                distance_metric='cosine'):\n","  people, name_to_idx = get_names(dataset)\n","  y_hat = clean_name(image_path)\n","  distances = np.zeros(len(people))\n","  heap_modified = np.zeros(len(people))\n","  \n","  for image_dict in dataset:\n","    if is_diff(image_dict['name'], image_path):\n","      continue\n","      \n","    name = clean_name(image_dict['name'])\n","    d = distance(embedding, image_dict['embedding'], distance_metric=distance_metric)\n","    idx = name_to_idx[name]\n","    \n","    if heap_modified[idx] == 0:\n","      heap_modified[idx] = 1\n","      distances[idx] = d\n","    else:\n","      if distances[idx] > d:\n","        distances[idx] = d\n","        \n","#   idx_min = distances.argmin()\n","  idxs = np.argsort(distances)[:5]\n","  y = [people[idx] for idx in idxs]  \n","  match = y_hat in y\n","#   y = people[idx_min]\n","#   match = y_hat == y\n","  return match, y, y_hat"],"execution_count":0,"outputs":[]},{"metadata":{"id":"El-y2hbIccOb","colab_type":"code","colab":{}},"cell_type":"code","source":["x = np.array([15, 12, 10, 2])\n","np.argsort(x)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"rOtW9jqhYCpO","colab":{}},"cell_type":"code","source":["# def predict_face(image_path, \n","#                  embedding, \n","#                  dataset, \n","#                  threshold=.1, \n","#                  distance_metric='cosine',\n","#                  output_folder='output',\n","#                  base_path='/gdrive/My Drive/FaceRecognition/datasets/lfw/lfw_mtcnnpy_160'):\n","#   people = get_names(dataset)\n","#   y_hat = clean_name(image_path)\n","#   distances = []\n","  \n","#   assert y_hat in people, \"The picture is not a part of the test set.\"\n","  \n","#   for person in people:\n","#     person_path = os.path.join(base_path, person)\n","#     person_path = os.path.join(person_path, output_folder)\n","#     faces = os.listdir(person_path)\n","#     faces = [os.path.join(person_path, face) for face in faces]\n","    \n","#     if person == y_hat:\n","#       faces = list(filter(lambda x: is_diff(x, image_path), faces))\n","      \n","#     temp_dist = []\n","#     for face in faces:\n","#       face_embedding = np.load(face)\n","#       d = distance(embedding, face_embedding, distance_metric=distance_metric)\n","#       temp_dist.append(d)\n","      \n","#     distances.append(min(d))\n","    \n","#   distances = np.array(distances)\n","#   min_distance_idx = distances.argmin()\n","#   y = people[min_distance_idx]\n","  \n","#   return y == y_hat, y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"idEmzIib2RCs","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"LQ2WseIS2kVy","colab_type":"code","colab":{}},"cell_type":"code","source":["os.chdir('/gdrive/My Drive/FaceRecognition/src')\n","import utils\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hWB4h-CX3aqc","colab_type":"code","colab":{}},"cell_type":"code","source":["sess = tf.Session()\n","utils.load_model('/gdrive/My Drive/FaceRecognition/models/facenet/20180402-114759/20180402-114759.pb')\n","\n","# Placeholders\n","images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n","embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n","phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n","embedding_size = embeddings.get_shape()[1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"da0qC6Df5H2Q","colab_type":"code","colab":{}},"cell_type":"code","source":["pred_true = 0\n","name_true = []\n","length = X_test.shape[0]\n","\n","for image_path in progressbar.progressbar(X_test):\n","  image = misc.imread(image_path)\n","  \n","  feed_dict = { images_placeholder: image.reshape((1, *image.shape)), \n","                phase_train_placeholder: False }\n","  embeddings_array = np.zeros((1, embedding_size))\n","  embeddings_array = sess.run(embeddings, feed_dict=feed_dict)\n","  \n","  match, prediction, _ = predict_face(image_path, embeddings_array, dataset, threshold=0.1)\n","  \n","  if match:\n","    pred_true += 1\n","    name_true.append(clean_name(image_path))\n","  \n","acc = pred_true / length\n","\n","print('[ACCURACY] %.2f%%' % (acc*100))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"POBBiDTmhrAF","colab_type":"code","colab":{}},"cell_type":"code","source":["name_true"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kjXAGEUV8obc","colab_type":"text"},"cell_type":"markdown","source":["## Analyse the test set"]},{"metadata":{"id":"kpFRMynb10wf","colab_type":"code","colab":{}},"cell_type":"code","source":["X_test = np.load('/gdrive/My Drive/FaceRecognition/datasets/lfw/xtest.npy')\n","y_test = np.load('/gdrive/My Drive/FaceRecognition/datasets/lfw/ytest.npy')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QP4kF-AN3EpS","colab_type":"code","colab":{}},"cell_type":"code","source":["im_path = X_test[628]\n","image = misc.imread(im_path)\n","  \n","feed_dict = { images_placeholder: image.reshape((1, *image.shape)), \n","              phase_train_placeholder: False }\n","embeddings_array = np.zeros((1, embedding_size))\n","embeddings_array = sess.run(embeddings, feed_dict=feed_dict)\n","\n","match, prediction, person = predict_face(im_path, embeddings_array, dataset, threshold=0.1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OZe5clLH3nDl","colab_type":"code","colab":{}},"cell_type":"code","source":["person"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LN7jf5lfjpjO","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"EZ5Y9Hn0i7hJ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"pr1QfLdUeO5b","colab_type":"code","colab":{}},"cell_type":"code","source":["prediction"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aBJulVfkeenE","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"7lRZinhUeVB3","colab_type":"code","colab":{}},"cell_type":"code","source":["os.chdir('/gdrive/My Drive/FaceRecognition/models/mobile-net/best_model')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NXl1gpR1qhy_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"388b117b-7d83-4deb-a9e4-b5767b9ec714","executionInfo":{"status":"ok","timestamp":1544706210976,"user_tz":120,"elapsed":648,"user":{"displayName":"Pedro Prates","photoUrl":"https://lh6.googleusercontent.com/-ogAT-E4sfMA/AAAAAAAAAAI/AAAAAAAAAa4/9MA7SqDoyXw/s64/photo.jpg","userId":"18091520408936459745"}}},"cell_type":"code","source":["os.listdir()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['mobilenetv1_v12.h5', 'mobilenetv1_v12.npy']"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"VC2zYhPzqdO0","colab_type":"code","colab":{}},"cell_type":"code","source":["model = models.load_model('mode')"],"execution_count":0,"outputs":[]}]}